<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>深入深出Sigmoid与Softmax的血缘关系 | 夕小瑶的科技屋</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文摘要：本文由逻辑回归模型的参数开始，通过将参数拆分为隐含的类别向量来探究sigmoid函数的本质意义，进而由sigmoid的意义推广出softmax。">
<meta property="og:type" content="article">
<meta property="og:title" content="深入深出Sigmoid与Softmax的血缘关系">
<meta property="og:url" content="http://xixiaoyao.github.io/2017/03/31/sigmoid-and-softmax/index.html">
<meta property="og:site_name" content="夕小瑶的科技屋">
<meta property="og:description" content="本文摘要：本文由逻辑回归模型的参数开始，通过将参数拆分为隐含的类别向量来探究sigmoid函数的本质意义，进而由sigmoid的意义推广出softmax。">
<meta property="og:image" content="https://ww2.sinaimg.cn/large/006tNc79gy1fe534xd78xj30go09owew.jpg">
<meta property="og:image" content="https://ww4.sinaimg.cn/large/006tNc79ly1fe57zn7m0zj306n01wwej.jpg">
<meta property="og:updated_time" content="2017-04-03T16:52:49.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深入深出Sigmoid与Softmax的血缘关系">
<meta name="twitter:description" content="本文摘要：本文由逻辑回归模型的参数开始，通过将参数拆分为隐含的类别向量来探究sigmoid函数的本质意义，进而由sigmoid的意义推广出softmax。">
<meta name="twitter:image" content="https://ww2.sinaimg.cn/large/006tNc79gy1fe534xd78xj30go09owew.jpg">
  
    <link rel="alternate" href="/atom.xml" title="夕小瑶的科技屋" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">夕小瑶的科技屋</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://xixiaoyao.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-sigmoid-and-softmax" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/31/sigmoid-and-softmax/" class="article-date">
  <time datetime="2017-03-31T12:55:51.000Z" itemprop="datePublished">2017-03-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/人工智能/">人工智能</a>►<a class="article-category-link" href="/categories/人工智能/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深入深出Sigmoid与Softmax的血缘关系
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文摘要：<br>本文由逻辑回归模型的参数开始，通过将参数拆分为隐含的类别向量来探究sigmoid函数的本质意义，进而由sigmoid的意义推广出softmax。</p>
<a id="more"></a>
<hr>
<p><u>声明：用于个人学习的转载无需告知，<strong>但请务必在文首注明出处！</strong>微信公众号、微博等平台的<strong>商业转载</strong>请务必联系本文作者<a href="https://xixiaoyao.github.io/about/">夕小瑶</a>。</u></p>
<h1 id="缘起逻辑回归"><a href="#缘起逻辑回归" class="headerlink" title="缘起逻辑回归"></a>缘起逻辑回归</h1><p>逻辑回归模型是用于二类分类的机器学习模型（不要说逻辑回归可以做多类分类啊喂，那是二类分类器的组合策略问题，而与逻辑回归分类器本身的构造没有半毛钱关系啊）。</p>
<!-- more -->
<p>我们知道，在逻辑回归中，用于预测样本类别的假设函数为</p>
<p>$$h_{\theta}(x)=sigmoid(\theta\cdot x)$$</p>
<p>（小夕要讲大事，忽略偏置项参数和向量转置这种细节啦）,其中sigmoid函数的图像看起来是这样的：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNc79gy1fe534xd78xj30go09owew.jpg" alt=""></p>
<p>因此，我们将$h_{\theta}(x)&gt;0.5$的样本预测为正类别（记为类别1），将$h_{\theta}(x)&lt;0.5$的样本预测为负类别（记为类别0）。因此对于sigmoid(z)函数来说，z=0的点就是用来分类的临界点。所以在逻辑回归中，$\theta\cdot x=0$的点就是分类的临界点。</p>
<p>可是你有想过为什么吗？（是的，这并不是拍脑袋决定的）</p>
<p>如果觉得小夕的这种问法很奇怪，那小夕换一种问法，你知道$\theta\cdot x$是代表什么意思吗？它难道仅仅代表了“特征向量与模型参数做内积”这么肤浅的含义吗？</p>
<p>听小夕慢慢讲，手指慢慢划，跟上思路哦。</p>
<p>首先，模型参数$\theta$是个向量，维数与样本的维数一致（忽略偏置项这种细节问题啦）,为了好看，<strong>下文用w来代替$\theta$</strong>。</p>
<p>我们来好好看看这个所谓的模型参数w。这个w在本质上是$w_{y=1}-w_{y=0}$，记为$\Delta w$。诶？怎么能这样呢？如何理解被拆出来的这两个w呢？</p>
<p>其实只要把$w_{y=1}$这个向量看作是对类别1的<strong>直接描述</strong>，将向量$w_{y=1}$看作是对类别0的直接描述，新的大门就打开了。还记得前面小夕讲的，在逻辑回归模型中，本质上用来预测类别的临界点就是$\theta\cdot x$，也就是$w_1\cdot x-w_0\cdot x$，这代表什么意思呢？</p>
<blockquote>
<p>我们知道，对于向量a和向量b，假设它们的长度都为1，那么当向量a与向量b夹角最小时，它们的内积，也就是$a\cdot b$会最大。当然了，推广到更一般的说法，不限制a与b的长度，则当a与b夹角最小时，我们称a与b的余弦相似度最大</p>
</blockquote>
<p>而两向量的夹角越小意味着什么呢？意味着这两个向量越相似呀，意味着越亲密呀。所以$w_1x-w_0x$就意味着类别1与特征向量x的亲密度减去类别0与x的亲密度。因此当逻辑回归的假设函数$h_\theta(x)&gt;0.5$时，也就是$w\cdot x&gt;0$时，就代表着特征向量x，也就是样本，与类别1更亲密，因此类别预测为1。同样的道理，当x与类别0更亲密时，类别预测为0。</p>
<p>继续，我们将上述神奇的逻辑放到逻辑回归模型的假设函数$h_\theta(x)$的展开式中，将$\theta$替换为我们上面的$\Delta w$，得：</p>
<p>$$h_\theta(x)=\frac{e^{\Delta w\cdot x}}{1+e^{\Delta w\cdot x}}$$</p>
<p>等等，有没有惊恐的发现什么？还记得小夕在上一篇文章<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484011&amp;idx=1&amp;sn=42e4f331db843091c5c3809a4d259fad&amp;chksm=970c2abda07ba3abb3963c2defcc644582f28bbdc23f3d669d022cd032e637d2ca8b6b48ca62#rd" target="_blank" rel="external">《逻辑回归》</a>中得到的这个结论吗？：</p>
<blockquote>
<p>我们再把$x^TΔw$整理的好看一点，变成更正常的形式：$w·x+b$。然后就可以得到下面的结论！！！：</p>
<p>$$P(Y=1|x)=\frac{e^{wx+b}}{1+e^{wx+b}}$$</p>
</blockquote>
<p>天呐，逻辑回归的假设函数竟然与$P(Y=1|X)$一模一样！都是$sigmoid(\Delta w\cdot x)$！！这个sigmoid函数到底是什么？一切真的都是因为巧合吗？不行，小夕非要一探究竟！来，手术刀拿来，解剖！</p>
<h1 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h1><p>为了美观，我们直接用$w1$代替$w_{y=1}$，用$w0$代替$w_{y=0}$</p>
<p>$$sigmoid(\Delta w\cdot x)=\frac{e^{\Delta w\cdot x}}{1+e^{\Delta w\cdot x}}=\frac{e^{(w1-w0)\cdot x}}{e^{(w0-w0)\cdot x}+e^{(w1-w0)\cdot x}}$$</p>
<p>如果我们令分子分母同除以$e^{-w0\cdot x}$。。。得：</p>
<p>$$sigmoid(\Delta w\cdot x)=\frac{e^{w1\cdot x}}{e^{w0\cdot x}+e^{w1\cdot x}}$$</p>
<p>！！！有没有被震惊到！小夕在前面讲了，w1与x的内积代表着w1与x的亲密度，这个不就代表着<strong>“类别1与x的亲密度 占 x与所有类别亲密度之和 的比例”</strong>吗？既然是比例，那肯定是0到1之间的数呀~而这个比例又可以解读为什么呢？不就是类别1在x心中的分量吗？当类别1在x心中的分量超过类别0在x心中的分量时，我们的逻辑回归模型当然要把类别1嫁给x呀~也就是将类别1作为预测的类别！同时，这个分量越大，我们将类别1嫁给x后，会让x满意的概率就越大！所以这个比例又是类别1的后验概率P(y=1|x)呀！</p>
<p>一切都不是巧合吧。Sigmoid函数的意义，竟然如此深邃。等等，虽然sigmoid(w1·x)代表”类别1与x的亲密度占x与所有类别亲密度之和的比例”，但是显然这里只有两个类别，即1和0，也就是说Sigmoid是一个只能用于<strong>二类分类</strong>的函数，那么如果我们要分类的类别超过2，我们能不能同样用一个函数来表示出“某类别与x的亲密度占x与所有类别亲密度之和的比例”呢？</p>
<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><p>这一次，我们倒着来！假如我们的分类任务有k个类别，与前面用w1、w0来表示类别1、类别2一样，我们用w1、w2、w3…wk来表示各个类别。</p>
<p>根据前面的经验，这个“类别j与特征向量x的亲密度”貌似可以表示为$e^{wj\cdot x}$，那么我们效仿一下sigmoid，类别j与x的亲密度占x与所有类别亲密度之和的比例即:</p>
<p>$$\frac{e^{wj\cdot x}}{e^{w1\cdot x}+e^{w2\cdot x}+e^{w3\cdot x}+…+e^{wk\cdot x}}$$</p>
<p>将分母用$\sum$整理一下，发现了没有！这就是大名鼎鼎的Softmax函数：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tNc79ly1fe57zn7m0zj306n01wwej.jpg" alt=""></p>
<p>哎，原来看似深不可测的Softmax函数，也就这样啦╮(╯▽╰)╭怪小夕咯？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xixiaoyao.github.io/2017/03/31/sigmoid-and-softmax/" data-id="cj12fppvt0004oq4at7sdqwmq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sigmoid/">sigmoid</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/softmax/">softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/逻辑回归/">逻辑回归</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/03/ateacher/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          一位老师，一位领导，一个让全体学生考上目标学校的故事
        
      </div>
    </a>
  
  
    <a href="/2017/03/31/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/机器学习/">机器学习</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Adaboost/">Adaboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sigmoid/">sigmoid</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/逻辑回归/">逻辑回归</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Adaboost/" style="font-size: 10px;">Adaboost</a> <a href="/tags/sigmoid/" style="font-size: 10px;">sigmoid</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/逻辑回归/" style="font-size: 10px;">逻辑回归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/04/03/ateacher/">一位老师，一位领导，一个让全体学生考上目标学校的故事</a>
          </li>
        
          <li>
            <a href="/2017/03/31/sigmoid-and-softmax/">深入深出Sigmoid与Softmax的血缘关系</a>
          </li>
        
          <li>
            <a href="/2017/03/31/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Xiaoyao Xi<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>